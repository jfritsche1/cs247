# -*- coding: utf-8 -*-
#%%
"""CS247_FinalProject_PreliminaryAnalysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lsMFFGkaqy-IeI-U6rwO6afDODK62rQn
"""

# !pip install vaderSentiment
# !pip install ekphrasis
# !pip install tweet-preprocessor

import os
import time
import pandas as pd
from sklearn.model_selection import train_test_split

import numpy as np
from pandas import DataFrame
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
from scipy import stats
import statsmodels.api as sm
from pprint import pprint
from datetime import datetime
from collections import Counter, defaultdict
import re, string, unicodedata
import json

from nltk import word_tokenize, sent_tokenize, FreqDist
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk.tokenize import TweetTokenizer

# import nltk
# nltk.download
# nltk.download('stopwords')
# nltk.download('punkt')
# nltk.download('wordnet')

from nltk.corpus import wordnet, stopwords
from wordcloud import WordCloud

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

import preprocessor as p

from view.plot_adjusted_closing_price import *
from view.plot_word_frequency import Plot_Word_Frequency
from controller.ticker_col_df import ticker_col_dataframe
from controller.text_preprocessing import text_preprocessing
from models.vader_sentiment import vader_sentiment
from models.finbert_analysis import run_finbert

plotGraphs = False

#stock_data
# !wget https://raw.githubusercontent.com/jfritsche1/cs247/main/Data/stock_data.csv?token=AMKNVA5FDCIM6SXH2WNOCVTAVCVJU -O stock_data.csv 
# !wget https://raw.githubusercontent.com/jfritsche1/cs247/main/Data/stock_data.csv?token=AMKNVA2LF3M74YEMW6H4PVLAWSXG4 -O stock_data.csv

#ticker name dataset
# !wget https://raw.githubusercontent.com/jfritsche1/cs247/main/Data/ticker_names.csv?token=AMKNVA627RHIRVXPHMKHQE3AVCVM2 -O ticker_names.csv
# !wget https://raw.githubusercontent.com/jfritsche1/cs247/main/Data/ticker_names.csv?token=AMKNVA2C3LJQ3CSVF7BHNQ3AWSXLE -O ticker_names.csv

#labeled twitter data
# !wget https://raw.githubusercontent.com/jfritsche1/cs247/main/Data/tweets_labelled_09042020_16072020.csv?token=AMKNVA7B2R2PUXT6A7LI47LAVCVRC -O tweets_labelled_09042020_16072020.csv
# !wget https://raw.githubusercontent.com/jfritsche1/cs247/main/Data/tweets_labelled_09042020_16072020.csv?token=AMKNVAYYGZV2Q2YEQS4Y2STAWSXO2 -O tweets_labelled_09042020_16072020.csv

#Upload tweets_remaining_09042020_16072020.zip
# from google.colab import files
# uploaded = files.upload()

# !unzip tweets_remaining_09042020_16072020.zip
data_path = os.path.join(".\data\sentiment_data\our_data")
stock_file = os.path.join(data_path,'stock_data.csv')
ticker_file = os.path.join(data_path,'ticker_names.csv')
twitter_labeled_file = os.path.join(data_path,'tweets_labelled_09042020_16072020.csv')
twitter_unlabeled_file = os.path.join(data_path,'tweets_remaining_09042020_16072020.csv')

# Read in stock data
stock_data = pd.read_csv(stock_file)
ticker_names = pd.read_csv(ticker_file)
grouped = DataFrame(ticker_names.groupby('Category').agg({'Symbol':lambda x: list(x)}))
tickers = ticker_names.set_index('Symbol').T.to_dict('records')[0]

# print (stock_data.head(5))
# print (ticker_names.head(5))
# print(grouped)

df_grouped=grouped.copy()
df_grouped = df_grouped.reset_index(col_level=0)
categories = df_grouped.set_index('Category').T.to_dict('records')[0]

#Reading twitter data
twitter_labeled = pd.read_csv(twitter_labeled_file, sep=';').set_index('id')
twitter_unlabeled = pd.read_csv(twitter_unlabeled_file, sep=';').set_index('id')

if (plotGraphs):
    PrintSummaryStatistics(stock_data, tickers)
    AdjCloseBoxPlot(stock_data)
    AdjClosePricePerCategoryBoxPlot(stock_data, categories)
    AdjClosePricePerCategoryDaily(stock_data, categories, tickers)

    # Inspect Twitter Data
    print(twitter_labeled.head(5))
    twitter_labeled.shape

    print(twitter_unlabeled.head(5))
    twitter_unlabeled.shape

    #Now, we are going to take a look at the twitter data
    #First 3 Tweets in the labeled dataset
    for i in range(3):
        pprint(twitter_labeled.iat[i,1])

    Plot_Word_Frequency(twitter_labeled,'text',"Labeled Data")
    Plot_Word_Frequency(twitter_unlabeled,'full_text', "Unlabeled Data")

#%%
#new dataframe for labeled dataset and create .csv file

new_twitter_labeled = ticker_col_dataframe(twitter_labeled, tickers,"text")
new_twitter_labeled.to_csv("new_labeled.csv")

#new dataframe for unlabeled dataset and create .csv file
#It takes about >50 minutes to run.  There is a zip file containing new_unlabeled.csv in Github

#new_twitter_unlabeled = ticker_col_dataframe(twitter_unlabeled, tickers, "full_text")
#new_twitter_unlabeled.to_csv("new_unlabeled.csv")

#%%
processed_labeled = text_preprocessing(twitter_labeled,'text','text')

#%%
#Vader sentiment analysis for labeled dataset
twitter_labeled_sentiments_score = vader_sentiment(processed_labeled,'text')
twitter_labeled_sentiments_score.head()

#%%
processed_unlabeled = text_preprocessing(twitter_unlabeled,'text','full_text')
for i in range(5):
    pprint(processed_unlabeled.iat[i,2])
 

#Vader sentiment analysis for unlabeled dataset
twitter_unlabeled_sentiments_score = vader_sentiment(processed_unlabeled,'text')
twitter_unlabeled_sentiments_score.head()

#%%
# Save the dataframe info to a tab seperated csv file (tsv) 
# for the split into train, test, and validation sets.

# temp = processed_labeled[['text','sentiment']].copy()
temp2 = temp.dropna(subset=['sentiment'])
cSEP_VAL = '.@'
temp2['sentiment'] = cSEP_VAL + temp2['sentiment'].astype(str)

#%%
import csv
outpath = os.path.join(data_path,"labeled_data.csv")
temp2.to_csv(outpath, sep=';', index=False, header=False, encoding='utf-8')

# Here is the function that will be used to read this data in.
# train = pd.read_csv(os.path.join(self.config.data_dir, 'train.csv'),sep='\t',index_col=False)

#%%
# data = pd.read_csv(data_path, sep=cSEP_VAL, names=['text','label'])
# train, test = train_test_split(data, test_size=0.2, random_state=0)

train, test = train_test_split(temp2, test_size=0.2, random_state=0)
train, valid = train_test_split(train, test_size=0.1, random_state=0)

train.to_csv('data/sentiment_data/train.csv',sep='\t',encoding='utf-8')
test.to_csv('data/sentiment_data/test.csv',sep='\t',encoding='utf-8')
valid.to_csv('data/sentiment_data/validation.csv',sep='\t',encoding='utf-8')

#%%
run_finbert(outpath)

#%%
labeled_data = text_preprocessing(new_twitter_labeled,'text','text')
temp = labeled_data.copy()

# Add a label column with just the date
temp['date'] = pd.to_datetime(temp['created_at'], format="%Y-%m-%d").dt.date

#%%
grouped_tweets = temp.groupby('date')
tweet_growth_days = grouped_tweets.sum()

#%%
# dataset_dates = pd.date_range(temp['created_at'][:].min(), temp['created_at'][:].max(), freq="D", normalize=True)
# bins = dataset_dates.date.tolist()
# tweet_growth_days['day'] = pd.cut(tweet_growth_days['date'], bins)

# display(df)
# %%
scores = []
for ticker in tweet_growth_days.items():
    test = tweet_growth_days[tweet_growth_days[ticker] > 0]
    
    pos_score = 0
    neg_score = 0
    if tweet_growth_days['sentiment'] == 'positive':
        pos_score += 1
    if tweet_growth_days['sentiment'] == 'negative':
        neg_score += 1
    scores.append({
        'Date': day,
        'Sentiment': pos_score - neg_score
    })
    